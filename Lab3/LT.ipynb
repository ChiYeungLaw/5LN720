{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SST2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_label = [], []\n",
    "with open('train.tsv', 'r', encoding='utf-8') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        train_text.append(line.strip().split('\\t')[0].strip())\n",
    "        train_label.append(int(line.strip().split('\\t')[1]))\n",
    "dev_text, dev_label = [], []\n",
    "with open('dev.tsv', 'r', encoding='utf-8') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        dev_text.append(line.strip().split('\\t')[0].strip())\n",
    "        dev_label.append(int(line.strip().split('\\t')[1].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT-distil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "config = DistilBertConfig.from_pretrained('bert-distil')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('bert-distil')\n",
    "bert_model = DistilBertModel.from_pretrained('bert-distil', config=config).cuda()\n",
    "for param in bert_model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert-distil + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def bert_embedding(x, bert_model, bert_tokenizer, device):\n",
    "    encode = bert_tokenizer(x, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "    input_ids, attention_mask = encode['input_ids'].to(device), encode['attention_mask'].to(device)\n",
    "    embed_x = bert_model(input_ids, attention_mask=attention_mask) # [bs, seq_len, 768]\n",
    "    return embed_x[0]\n",
    "\n",
    "class Att(nn.Module):\n",
    "    def __init__(self, device, input_dim=768, out_dim=2):\n",
    "        super(Att, self).__init__()\n",
    "        self.proj1 = nn.Linear(input_dim, input_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.proj2 = nn.Linear(input_dim, out_dim)\n",
    "        self.device = device\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        nn.init.xavier_uniform_(self.proj1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.proj2.weight.data)\n",
    "        nn.init.constant_(self.proj1.bias.data, 0.1)\n",
    "        nn.init.constant_(self.proj2.bias.data, 0.1)\n",
    "        nn.init.uniform_(self.u, -0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, bert_model, bert_tokenizer):\n",
    "        embed_x = bert_embedding(x, bert_model, bert_tokenizer, self.device) # [bs, seq_len, 768]\n",
    "        ut = self.tanh(self.proj1(embed_x)) # ut: [bs, seq_len, 768]\n",
    "        alpha = torch.softmax(torch.matmul(ut, self.u), dim=1) # alpha: [bs, seq_len, 1]\n",
    "        s = torch.sum(alpha * embed_x, dim=1) # s: [bs, 768]\n",
    "        return self.proj2(s), alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_text, train_label, bs, num_epoch, optimizer, criterion,\n",
    "          dev_text, dev_label, bert_model, bert_tokenizer, PATH, device):\n",
    "    bert_model.eval()\n",
    "    num_batch = len(train_text) // bs\n",
    "    max_dev_acc = 0.\n",
    "    for epoch in range(num_epoch):\n",
    "        tot_loss = 0.\n",
    "        model.train()\n",
    "        for i in range(num_batch):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = train_text[i*bs:(i+1)*bs], torch.LongTensor(train_label[i*bs:(i+1)*bs]).to(device)\n",
    "            pred, _ = model(x, bert_model, bert_tokenizer) # [bs, out_dim]\n",
    "            loss = criterion(pred, y)\n",
    "            tot_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        dev_acc = eval(model, dev_text, dev_label, bs, bert_model, bert_tokenizer)\n",
    "        if dev_acc > max_dev_acc:\n",
    "            max_dev_acc = dev_acc\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}, Total loss: {tot_loss:.4f}, Dev Acc.: {dev_acc:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dev_text, dev_label, bs, bert_model, bert_tokenizer):\n",
    "    correct, total = 0, 0\n",
    "    num_batch = len(dev_text) // bs\n",
    "    model.eval()\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batch):\n",
    "            x, y = dev_text[i*bs:(i+1)*bs], dev_label[i*bs:(i+1)*bs]\n",
    "            pred, _ = model(x, bert_model, bert_tokenizer)\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            for j in range(len(y)):\n",
    "                if pred[j].item() == y[j]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = Att(device=device,\n",
    "            input_dim=768,\n",
    "            out_dim=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Total loss: 455.4002, Dev Acc.: 84.01%.\n",
      "Epoch 2/10, Total loss: 388.0566, Dev Acc.: 82.21%.\n",
      "Epoch 3/10, Total loss: 352.2763, Dev Acc.: 82.45%.\n",
      "Epoch 4/10, Total loss: 321.0624, Dev Acc.: 82.81%.\n",
      "Epoch 5/10, Total loss: 293.0998, Dev Acc.: 81.37%.\n",
      "Epoch 6/10, Total loss: 271.4165, Dev Acc.: 83.05%.\n",
      "Epoch 7/10, Total loss: 254.6685, Dev Acc.: 83.77%.\n",
      "Epoch 8/10, Total loss: 238.5233, Dev Acc.: 81.97%.\n",
      "Epoch 9/10, Total loss: 224.2089, Dev Acc.: 83.05%.\n",
      "Epoch 10/10, Total loss: 214.3441, Dev Acc.: 82.93%.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_text, train_label, bs=64, num_epoch=10,\n",
    "      optimizer=optimizer, criterion=criterion,\n",
    "      dev_text=dev_text, dev_label=dev_label,\n",
    "      bert_model=bert_model, bert_tokenizer=tokenizer,\n",
    "      PATH='SST2-model-ATT.pt', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentiment Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"SST2-model-ATT.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(sent, model, bert_model, bert_tokenizer):\n",
    "    model.eval()\n",
    "    tok_sent = bert_tokenizer.tokenize(sent)\n",
    "    new_sent = []\n",
    "    p = 0\n",
    "    while p < len(tok_sent):\n",
    "        if \"##\" not in tok_sent[p]:\n",
    "            new_sent.append(tok_sent[p])\n",
    "        else:\n",
    "            new_sent[-1] += tok_sent[p][2:]\n",
    "        p += 1\n",
    "    with torch.no_grad():\n",
    "        _, scores = model([sent], bert_model, bert_tokenizer)\n",
    "        scores = scores.view(-1).cpu().numpy().tolist()\n",
    "        new_scores = []\n",
    "        p = 0\n",
    "        while p < len(scores):\n",
    "            if \"##\" not in tok_sent[p]:\n",
    "                if len(new_scores) > 0:\n",
    "                    new_scores[-1] /= count\n",
    "                new_scores.append(scores[p])\n",
    "                count, p = 1, p + 1\n",
    "            else:\n",
    "                new_scores[-1] += scores[p]\n",
    "                count, p = count + 1, p + 1\n",
    "        new_scores = np.array(new_scores)\n",
    "        \n",
    "        idx = []\n",
    "        for i in range(min(5, len(new_sent))):\n",
    "            idx.append(new_scores.argmax())\n",
    "            new_scores[idx[-1]] = -float('inf')\n",
    "    \n",
    "    #for i in range(len(idx)):\n",
    "    #    print(f\"{i+1}. {new_sent[idx[i]]}\")\n",
    "    return new_sent[idx[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment DIctionary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = set(), set()\n",
    "for i in range(len(train_text)):\n",
    "    if train_label[i] == 0:\n",
    "        neg.add(analysis(train_text[i], model, bert_model, tokenizer))\n",
    "    else:\n",
    "        pos.add(analysis(train_text[i], model, bert_model, tokenizer))\n",
    "pos_neg = pos & neg\n",
    "pos = pos - pos_neg\n",
    "neg = neg - pos_neg\n",
    "with open('pos.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in pos:\n",
    "        f.write(word + ', ')\n",
    "with open('neg.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in neg:\n",
    "        f.write(word + ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, true, new_text = [], [], []\n",
    "for i in range(len(dev_text)):\n",
    "    word = analysis(dev_text[i], model, bert_model, tokenizer)\n",
    "    f1, f2 = False, False\n",
    "    if word in pos:\n",
    "        f1 = True\n",
    "    if word in neg:\n",
    "        f2 = True\n",
    "    if (f1 and f2) or (not f1 and not f2):\n",
    "        continue\n",
    "    elif f1:\n",
    "        true.append(dev_label[i])\n",
    "        pred.append(1)\n",
    "        new_text.append(dev_text[i])\n",
    "    else:\n",
    "        pred.append(0)\n",
    "        true.append(dev_label[i])\n",
    "        new_text.append(dev_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5440414507772021"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct, tot = 0, 0\n",
    "for i in range(len(pred)):\n",
    "    if true[i] == 1:\n",
    "        correct += 1\n",
    "    tot += 1\n",
    "correct / tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATT Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8177083333333334"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(model, new_text, true, 32, bert_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564766839378239"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = []\n",
    "for sent in new_text:\n",
    "    tok_sent = tokenizer.tokenize(sent)\n",
    "    new_sent = []\n",
    "    p = 0\n",
    "    while p < len(tok_sent):\n",
    "        if \"##\" not in tok_sent[p]:\n",
    "            new_sent.append(tok_sent[p])\n",
    "        else:\n",
    "            new_sent[-1] += tok_sent[p][2:]\n",
    "        p += 1\n",
    "    res = [0, 0]\n",
    "    for word in new_sent:\n",
    "        if word in pos:\n",
    "            res[1] += 1\n",
    "        if word in neg:\n",
    "            res[0] += 1\n",
    "    if res[0] >= res[1]:\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)\n",
    "correct, tot = 0, 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == true[i]:\n",
    "        correct += 1\n",
    "    tot += 1\n",
    "correct / tot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
